{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93a4cc04",
   "metadata": {},
   "source": [
    "# Workshop Time Series\n",
    "\n",
    "In this workshop you will practice performing time series analysis.\n",
    "\n",
    "Lesson goal: The student applies time series forecasting techniques to make predictions using a time series data set. This lesson goal relates to final qualification \"data analytics\".\n",
    "\n",
    "\n",
    "We will be using a dataset containing energy consumption data for a heat pump. It contains the following columns:\n",
    "\n",
    "- timestamp: timestamp\n",
    "- E_apparaten: energy use of other devices in kWh\n",
    "- E_warmtepomp: energy use of heatpump in kWh\n",
    "- E_verbruik: total energy use in kWh\n",
    "- Overschot: energy returned to grid\n",
    "- hp_AAN: heatpump on or off\n",
    "- diff_temp_woonk: temperature living room\n",
    "- diff_temp_slaapk: temperature bedroom\n",
    "- Tussendeur: inside door open / closed\n",
    "- Buitendeur: outside door open / closed\n",
    "- Raam: window open / closed\n",
    "- Dag: weekday\n",
    "- day_of_week: weekday (same as \"Dag\")\n",
    "- month: month\n",
    "- year: year\n",
    "\n",
    "In this workshop we will focus on the E_warmtepomp column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26cc32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from statsmodels.graphics import tsaplots\n",
    "from statsmodels.tsa.stattools import acf\n",
    "import statsmodels.tsa.api as tsa\n",
    "from scipy.signal import savgol_filter, butter, filtfilt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6567d1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "dfhp = pd.read_csv(\"data/heatpump_clean_filtered.csv\")\n",
    "dfhp['timestamp_dt'] = pd.to_datetime(dfhp['timestamp']) # Create a copy of the timestamp that is converted to a proper date time stamp object so Python can analyze it better.\n",
    "dfhp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e2370f",
   "metadata": {},
   "source": [
    "## Initial analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fda7261",
   "metadata": {},
   "source": [
    "As there are 100,000 rows for our initial experiments for our first analyses we will summarize them or zoom on sub selections to avoid having to spend too much time waiting for calculations to complete. Later on you will work on the full data set.\n",
    "\n",
    "For starters, lets sum the E_warmptepomp data for each day. Then plot the contents to see what we're dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3927907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be able to sum by day we need to create a column that contains the day only.\n",
    "# Note we use the to_datetime function to convert the date string to something\n",
    "# that Python recognizes as a date object.\n",
    "dfhp['datum'] = pd.to_datetime(dfhp['timestamp'].replace(' .*$', '', regex=True))\n",
    "# Now sum the E_warmtepomp values for each day. Reset the index to obtain a dataframe instead of a groupby object.\n",
    "df_sum = dfhp[['datum', 'E_warmtepomp']].groupby(by='datum').sum().reset_index()\n",
    "\n",
    "plt.plot(df_sum['datum'], df_sum['E_warmtepomp'])\n",
    "plt.title('Heat pump energy use / day')\n",
    "plt.ylabel('Heat pump energy use (kWh)')\n",
    "plt.xlabel('Date')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a9d6b5",
   "metadata": {},
   "source": [
    "Clearly there are some interesting things going on with the data. For our first experiments, these are inconvenient, however, so let's first zoom in on a sub selection of data, say from 2020-01-01 to 2020-04-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5225e02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sum_sel = df_sum[(df_sum['datum'] >= '2020-01-01') & (df_sum['datum'] < '2020-04-01')].reset_index(drop=True) # reset index is needed for ARIMA, later on.\n",
    "plt.plot(df_sum_sel['datum'], df_sum_sel['E_warmtepomp'])\n",
    "plt.title('Heat pump energy use / day between 2020-01-01 and 2020-04-01')\n",
    "plt.ylabel('Heat pump energy use (kWh)')\n",
    "plt.xlabel('Date')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da637cb7",
   "metadata": {},
   "source": [
    "There does not seem to be an obvious pattern here. Let's create an autocorrelation function plot to see if this intuition is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4f9871",
   "metadata": {},
   "outputs": [],
   "source": [
    "acf_plot = tsaplots.plot_acf(df_sum_sel[\"E_warmtepomp\"], lags=30, title='Autocorrelation plot (heat pump energy use / day between 2020-01-01 and 2020-04-01)')\n",
    "\n",
    "# Adjust the plot size\n",
    "acf_plot.set_figheight(5)\n",
    "acf_plot.set_figwidth(10)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf91963b",
   "metadata": {},
   "source": [
    "Autocorrelation is extremely weak, confirming our intuition that there is no obvious repeating pattern in the data.\n",
    "\n",
    "However, there may be a seasonal effect in the data, albeit a weak one. Can you spot it?\n",
    "\n",
    "Let's see if decomposition makes the effect more obvious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f44e751",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Decompose the time series\n",
    "result = seasonal_decompose(df_sum_sel[\"E_warmtepomp\"], model=\"additive\", period=30)\n",
    "\n",
    "# Plot the decomposed components\n",
    "decomposed = result.plot()\n",
    "\n",
    "decomposed.set_figheight(8)\n",
    "decomposed.set_figwidth(10)\n",
    "decomposed.suptitle(\"Time Series Decomposition (heat pump energy use / day between 2020-01-01 and 2020-04-01)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22876edd",
   "metadata": {},
   "source": [
    "There seems to be a clear, albeit small, trend in the data. Perhaps filtering would make this effect more obvious?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7e7714",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_frequency = 0.1  # Cutoff frequency as a fraction of the sampling rate\n",
    "filter_order = 3  # Filter order\n",
    "b, a = butter(filter_order, cutoff_frequency, btype=\"low\", analog=False)\n",
    "butterworth_filtered = filtfilt(b, a, df_sum_sel['E_warmtepomp'])\n",
    "\n",
    "plt.plot(df_sum_sel['datum'], butterworth_filtered)\n",
    "plt.title('Heat pump energy use / day between 2020-01-01 and 2020-04-01 (filtered)')\n",
    "plt.ylabel('Heat pump energy use (kWh)')\n",
    "plt.xlabel('Date')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6297921f",
   "metadata": {},
   "source": [
    "Play around with the cutoff_frequency parameter to see if you can make the effect more pronounced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483b0c1a",
   "metadata": {},
   "source": [
    "## Try it yourself: analyze the full data set\n",
    "\n",
    "Now try it yourself. Using the complete data set, analyse it for seasonality and trends. Make sure to use the dfhp['timestamp_dt'] column for the timestamp.\n",
    "\n",
    "Play around with the values for the lags parameter and the period parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fade150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for the ACF plot goes here. Try changing the lags parameter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fcf374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for the decomposition goes here. Be sure to try changing the period parameter.\n",
    "# If you have time, try filtering the data before decomposing it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbce22b",
   "metadata": {},
   "source": [
    "## Differencing\n",
    "\n",
    "Our data has a (weak) trend. We can get rid of this trend by differencing. Doing so manually would be possible (refer to the lecture demos for an example) but Pandas has a differencing function built in so let's use that.\n",
    "\n",
    "For the example we will again use the summarized data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696893f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without a value for the periods parameter it defaults to 1\n",
    "sum_diff = df_sum_sel['E_warmtepomp'].diff(periods=1)\n",
    "plt.plot(df_sum_sel['datum'], sum_diff)\n",
    "plt.title('Differenced heat pump energy use / day between 2020-01-01 and 2020-04-01')\n",
    "plt.ylabel('Difference')\n",
    "plt.xlabel('Date')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282f7566",
   "metadata": {},
   "source": [
    "Whereas before we had a slight downward trend after differencing that trend is gone.\n",
    "\n",
    "To verify that the autocorrelation is now (mostly) gone, we can make an autocorrelation function plot for the differenced time series.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76354c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to dropna as the first value is now empty (nothing to diff with!)\n",
    "acf_plot = tsaplots.plot_acf(df_sum_sel[\"E_warmtepomp\"].diff().dropna(), lags=30, title='Autocorrelation plot (differenced heat pump energy use / day between 2020-01-01 and 2020-04-01)')\n",
    "\n",
    "# Adjust the plot size\n",
    "acf_plot.set_figheight(5)\n",
    "acf_plot.set_figwidth(10)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cfd993",
   "metadata": {},
   "source": [
    "Correlations, especially the second one, are now much weaker. The trend may be too, but this is difficult to see as it was quite weak to begin with.\n",
    "\n",
    "Let's try to decompose the differenced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c272318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose the time series\n",
    "result = seasonal_decompose(df_sum_sel[\"E_warmtepomp\"].diff().dropna(), model=\"additive\", period=30)\n",
    "\n",
    "# Plot the decomposed components\n",
    "decomposed = result.plot()\n",
    "\n",
    "decomposed.set_figheight(8)\n",
    "decomposed.set_figwidth(10)\n",
    "decomposed.suptitle(\"Time Series Decomposition (differenced)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984f668f",
   "metadata": {},
   "source": [
    "The trend is clearly gone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa0ead4",
   "metadata": {},
   "source": [
    "## Try it yourself: differencing\n",
    "\n",
    "Try differencing the entire data set. Does this remove the trend? And does this remove the seasonality, too?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08991d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1803d98d",
   "metadata": {},
   "source": [
    "## Forecasting\n",
    "\n",
    "Now that we have analyzed our time series we can attempt to make forecasts.\n",
    "\n",
    "We will use the ARIMA model from the statsmodels.tsa.api package. Perhaps confusingly this is also the model to use for ARMA models - by setting the differencing order to 0 (which of course makes sense).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1904a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the ARIMA model\n",
    "num_previous_terms = 1 # Commonly called p\n",
    "differencing_order = 0 # Commonly called d. Should be 0 for ARMA\n",
    "num_terms_moving_average = 1 # Commonly called q\n",
    "\n",
    "model = tsa.ARIMA(df_sum_sel['E_warmtepomp'], order=(num_previous_terms, differencing_order, num_terms_moving_average))\n",
    "\n",
    "fitted_model = model.fit()\n",
    "\n",
    "fitted_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843f53db",
   "metadata": {},
   "source": [
    "Now that we have a model, we can make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5995a03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_values_to_forecast = 10 # defaults to 1\n",
    "print(fitted_model.forecast(num_values_to_forecast))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9b8a17",
   "metadata": {},
   "source": [
    "Let's eyeball the quality of our forecasts by plotting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114b70c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inc_forecasts = pd.concat([df_sum_sel['E_warmtepomp'], fitted_model.forecast(num_values_to_forecast)])\n",
    "data_inc_forecasts.plot()\n",
    "plt.title('Heat pump energy use inc prediction (ARMA)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed08611a",
   "metadata": {},
   "source": [
    "The forecasts are quite bad - a nearly straight line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348f1130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play around with these parameters until you get a reasonable looking result.\n",
    "num_previous_terms = 4 # Commonly called p\n",
    "differencing_order = 3 # Now set it to 1 - or even higher to get rid of higher order trends and seasonality\n",
    "num_terms_moving_average = 2 # Commonly called q\n",
    "\n",
    "model_arima = tsa.ARIMA(df_sum_sel['E_warmtepomp'], order=(num_previous_terms, differencing_order, num_terms_moving_average), enforce_invertibility=False)\n",
    "\n",
    "fitted_model_arima = model_arima.fit()\n",
    "\n",
    "data_inc_forecasts = pd.concat([df_sum_sel['E_warmtepomp'], fitted_model_arima.forecast(num_values_to_forecast)])\n",
    "data_inc_forecasts.plot()\n",
    "plt.title('Heat pump energy use inc prediction (ARIMA)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f315bf4a",
   "metadata": {},
   "source": [
    "Still not very good. Our data is simply too noisy to work well. How bad is it? To find this out, we need to split the data into a train set and a test set. FOR THIS WE DO NOT USE scikit-learn's train_test_split!\n",
    "\n",
    "Then we make predictions on the train set and compare those to the test set using mean_absolute_error and mean_absolute_percentage_error from sklearn.metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9569302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train set and a test set. Last 10 readings are the test set.\n",
    "train = df_sum_sel['E_warmtepomp'][:-10]\n",
    "test = df_sum_sel['E_warmtepomp'][-10:]\n",
    "\n",
    "# Play around with these parameters until you get a reasonable result.\n",
    "num_previous_terms = 1 # Commonly called p\n",
    "differencing_order = 2 # Now set it to 1 - or even higher to get rid of higher order trends and seasonality\n",
    "num_terms_moving_average = 1 # Commonly called q\n",
    "\n",
    "# Train a model\n",
    "model_arima = tsa.ARIMA(train, order=(num_previous_terms, differencing_order, num_terms_moving_average), enforce_invertibility=False)\n",
    "\n",
    "fitted_model_arima = model_arima.fit()\n",
    "\n",
    "forecasts = fitted_model_arima.forecast(test.size)\n",
    "\n",
    "# Calculate the MAPE\n",
    "print(f'The MAPE is {mean_absolute_percentage_error(test, forecasts) * 100:.0f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40918d86",
   "metadata": {},
   "source": [
    "Now try it yourself on the complete data set. Create a train set and a test set (manually!)\n",
    "\n",
    "For the parameters: play around with the number of previous terms and previous error terms to use. Note that larger values will lead to longer run times.\n",
    "\n",
    "Also, try different test set sizes. What does this tell you about the predictive power of this model?\n",
    "\n",
    "Finally, you may obtain better results if you focus on part of the data only, either the range before april or the range after (as there is a clear cutoff point in the first week of april)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4b2308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a00a7d",
   "metadata": {},
   "source": [
    "It is likely your results on the full data set, with 4 readings per hour, are far worse than the results for the summarized data set, with only one summed value per day, if you predict more than a handful of values.\n",
    "\n",
    "If you think about it, the reasons for this are obvious - but it is nonetheless disappointing.\n",
    "\n",
    "While time series forecasting can be a powerful technique, it only works on data sets where there is a meaningful relationship between data points. Often you will need to do additional work before such a relationship is produced, as we did in the examples above where we summed the individual readings per day."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
