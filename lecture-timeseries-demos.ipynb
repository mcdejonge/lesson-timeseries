{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a7a2d6c",
   "metadata": {},
   "source": [
    "# Lecture Timeseries (1): Examples\n",
    "\n",
    "This notebook contains examples for the lecture Timeseries (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053867ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from statsmodels.graphics import tsaplots\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from scipy.signal import savgol_filter, butter, filtfilt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf92814",
   "metadata": {},
   "source": [
    "## Demo 1: naive forecast with example data\n",
    "\n",
    "This demo shows an example of a naive forecast using some synthetic data (numbers from 1 to 7), ordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1dd493",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "pd.Series(ts).plot()\n",
    "plt.title('Simple \"time series\" containing numbers from 1 to 7.')\n",
    "plt.xlabel(\"Observation #\")\n",
    "plt.ylabel(\"Observation value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169d0a5a",
   "metadata": {},
   "source": [
    "The next number in this series would be 8. If we were to use naive forecasting, we would predict 7. How bad is this prediction?\n",
    "\n",
    "The absolute error is 1 (of course). Is that good or bad? We don't know. Let's calculate the absolute percentage error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c07e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predicted next value in the series is:\n",
    "ts_next = ts[-1]\n",
    "\n",
    "# The real next value in the series is:\n",
    "real_ts_next = ts[-1] + 1\n",
    "\n",
    "# The absolute percentage error for the predicted value is:\n",
    "print(f\"The absolute percentage error of our prediction is {abs((ts_next - real_ts_next) / real_ts_next) * 100 :.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba63f1a1",
   "metadata": {},
   "source": [
    "Not terrible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625f1127",
   "metadata": {},
   "source": [
    "## Demo 2: manual ARMA\n",
    "\n",
    "Let's manually perform an ARMA prediction on the time series. We need  coefficients for this ($\\beta_1, \\beta_2, ..., \\beta_k$). Finding out what these are is usually done by training. For this example let's simply give the last observation a weight of 1, the next to last one a weight of 0.1 and all the other observations a weight of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94349e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [0 for i in ts]\n",
    "weights[-1] = 1\n",
    "weights[-2] = 0.1\n",
    "weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d517336",
   "metadata": {},
   "source": [
    "Calculating the residuals is a bit tricky. Basically for every value except the first we need to calculate what the error would have been (the residual). The method below for doing this works, but is of course not elegant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea333f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals  = [0] # The first residual is 0, because we cannot predict it\n",
    "predicted = [1] # We have to set the first value to 1, because we cannot predict it.\n",
    "for i in range(1, len(ts)):\n",
    "    prev = ts[i - 1]\n",
    "    prev_prev = ts[i - 2]\n",
    "    predicted.append((prev * 1) + (prev_prev * 0.1))\n",
    "    residuals.append(((prev * 1) + (prev_prev * 0.1)) - ts[i])\n",
    "\n",
    "print(f'Real observations:\\t{ts}')\n",
    "print(f'Predicted observations:\\t{predicted}')\n",
    "print(f'Residuals:\\t\\t{residuals}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53608613",
   "metadata": {},
   "source": [
    "Now let's combine the auto regression and the moving average (calculated by giving all but the last two values of the residuals a weight of 0) to predict the next value in the series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36552a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_weights = [0 for residual in residuals]\n",
    "ma_weights[-1] = 1\n",
    "ma_weights[-2] = 1\n",
    "\n",
    "value_to_predict = 8\n",
    "print(f'Weights for residuals:\\t{ma_weights}')\n",
    "\n",
    "# First calculate the prediction before adding residuals. This time we use the real values, not the predicted ones.\n",
    "arma_prediction_no_ma = 1\n",
    "for i in range(1, len(ts)):\n",
    "    arma_prediction_no_ma += ts[i] * weights[i]\n",
    "\n",
    "# Now add the residuals to the prediction.\n",
    "arma_prediction_complete = arma_prediction_no_ma\n",
    "for i in range(1, len(ts)):\n",
    "    arma_prediction_complete += residuals[i] * ma_weights[i]\n",
    "\n",
    "\n",
    "print(f\"Predicted value without residuals is {arma_prediction_no_ma}. The absolute percentage error for this prediction is {abs((arma_prediction_no_ma - value_to_predict)/ value_to_predict) * 100 :.2f}%\")\n",
    "\n",
    "print(f\"Predicted value with residuals is {arma_prediction_complete}. The absolute percentage error for this prediction is {abs((arma_prediction_complete - value_to_predict) / value_to_predict) * 100 :.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1847b8d4",
   "metadata": {},
   "source": [
    "As you can see, even in this simple and highly artificial case ARMA performs better than naive forecasting.\n",
    "\n",
    "NOTE: in \"real\" ARMA processing happens in reverse order (newest to oldest)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc518621",
   "metadata": {},
   "source": [
    "## Demo 3: manual ARMA, does not work\n",
    "\n",
    "ARMA makes a few assumptions about the data. Here we see an example where ARME does not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220a8871",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts2 = [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4]\n",
    "\n",
    "pd.Series(ts2).plot()\n",
    "plt.title('A different \"time series\" containing numbers, from 1 to 4 this time.')\n",
    "plt.xlabel(\"Observation #\")\n",
    "plt.ylabel(\"Observation value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3326574a",
   "metadata": {},
   "source": [
    "Once again, let's manually perform an ARMA prediction of the next value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180d6533",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights2 = [0 for i in ts2]\n",
    "weights2[-1] = 1\n",
    "weights2[-2] = 0.1\n",
    "print(f'Weights:\\t\\t{weights2}')\n",
    "residuals2  = [0] # The first residual is 0, because we cannot predict it\n",
    "predicted2 = [1] # For demonstration purposes\n",
    "for i in range(1, len(ts2)):\n",
    "    prev = ts2[i - 1]\n",
    "    prev_prev = ts2[i - 2]\n",
    "    predicted2.append((prev * 1) + (prev_prev * 0.1))\n",
    "    residuals2.append(((prev * 1) + (prev_prev * 0.1)) - ts2[i])\n",
    "\n",
    "print(f'Real observations:\\t{ts2}')\n",
    "print(f'Predicted observations:\\t{predicted2}')\n",
    "print(f'Residuals:\\t\\t{residuals2}')\n",
    "\n",
    "ma_weights2 = [0 for residual in residuals2]\n",
    "ma_weights2[-1] = 1\n",
    "ma_weights2[-2] = 1\n",
    "print(f'Weights for residuals:\\t{ma_weights2}')\n",
    "arma2 = 0\n",
    "for i in range(0, len(ts2)):\n",
    "    weight = weights2[i]\n",
    "    ma_weight = ma_weights2[i]\n",
    "    arma2 += ts2[i] * weight\n",
    "    arma2 += residuals2[i] * ma_weight\n",
    "\n",
    "print(f\"Predicted value is {arma2}. The absolute percentage error for this prediction is {abs((arma2 - ts2[-1]) / ts2[-1]) * 100 :.2f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53d6697",
   "metadata": {},
   "source": [
    "The reason the prediction is so far off is that ARMA assumes the time series is *stationary*, ie there is no so-called \"seasonality\" (recurring patterns) and overall trends in the data.\n",
    "\n",
    "The second example time series clearly has a recurring pattern, which is why ARMA does not work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7aa151",
   "metadata": {},
   "source": [
    "## Demo 4: reducing seasonality using differencing\n",
    "\n",
    "It is possible to remove overall trends and seasonality from a timeseries by using a technique called *differencing*. In this technique you calculate the difference (hence the name) between values in a time series and a shifted version of those values. This allows you to study the properties of the time series without the distraction of the seasonality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd7df7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons = [1, 2, 3, 4, 3, 2, 1]\n",
    "ts_noseasons = [1, 2, 1, 2, 1, 2, 1]\n",
    "ts3 = list(map(lambda season, value: season + value, seasons, ts_noseasons))\n",
    "\n",
    "pd.Series(seasons).plot(color='green', style='o-')\n",
    "pd.Series(ts_noseasons).plot(color='blue', style='^-')\n",
    "pd.Series(ts3).plot(color=\"gray\")\n",
    "plt.title('A time series containing seasonality.')\n",
    "plt.xlabel(\"Observation #\")\n",
    "plt.ylabel(\"Observation value\")\n",
    "plt.legend(['Seasonality', 'Without seasonality', 'Complete'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec0ca99",
   "metadata": {},
   "source": [
    "Let's manually perform differencing. We create a new time series that contains the difference between each value and the previous value (this gives us - roughly - the *differential* of the time series at each point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797831f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts3_shifted = ts3[-1:]\n",
    "ts3_shifted += ts3[:-1]\n",
    "print(f'Original: \\t{ts3}')\n",
    "print(f'Shifted: \\t{ts3_shifted}')\n",
    "differenced = list(map(lambda orig, prev: orig - prev, ts3, ts3_shifted))\n",
    "print(f'Difference: \\t{differenced}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df05b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(differenced).plot()\n",
    "plt.title('Differenced time series')\n",
    "plt.xlabel(\"Observation #\")\n",
    "plt.ylabel(\"Observation value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76623c9",
   "metadata": {},
   "source": [
    "This technique does not yield the underlying data without the seasonal trend (up and then down) as it was originally but it does give us a better view of it. You can clearly see the zig zag trend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abd417b",
   "metadata": {},
   "source": [
    "## Demo 5: using statsmodels.tsa to show trends and seasonality\n",
    "\n",
    "In practice, we do not, of course, write our own buggy code to work with time series. Instead we use the Python statsmodels.tsa library.\n",
    "\n",
    "(this example is copied from RaoulG's https://github.com/raoulg/MADS-DAV/blob/main/notebooks/03.2-statistics-of-time.ipynb)\n",
    "\n",
    "For this example we will again use synthetic data. We will create:\n",
    "\n",
    "- 3 years of data\n",
    "- with 4 seasonal cycles per year\n",
    "- we add a trend of +10 over the full period\n",
    "- and we add some noise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54cbae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a date range\n",
    "years = 3\n",
    "dates = pd.date_range(start=\"2020-01-01\", periods=365 * years, freq=\"D\")\n",
    "\n",
    "# Generate synthetic data with seasonality\n",
    "\n",
    "# First create a linear trend\n",
    "np.random.seed(42)\n",
    "trend = np.linspace(0, 10, len(dates))\n",
    "\n",
    "\n",
    "# Then add seasonality: for every year, create 4 cycles.\n",
    "cycles = 4\n",
    "seasonality = 10 * np.sin(\n",
    "    np.linspace(0, cycles * years * 2 * np.pi, len(dates))\n",
    ")  # Yearly seasonality\n",
    "\n",
    "\n",
    "\n",
    "# Now create random noise. This is our time series *without* the seasonality and trend.\n",
    "noise = np.random.normal(0, 1, len(dates))  # Random noise\n",
    "\n",
    "pd.Series(trend).plot(color=\"black\")\n",
    "pd.Series(seasonality).plot(color=\"grey\", linestyle=\"dashed\")\n",
    "pd.Series(noise).plot(linestyle=\"dotted\")\n",
    "plt.title('Elements of our time series')\n",
    "plt.legend(['Trend', 'Seasonality (4 cycles / year)', 'Noise'])\n",
    "plt.show()\n",
    "\n",
    "# Finally, combine the trend, the seasonality and the noise.\n",
    "synthetic_data = trend + seasonality + noise\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\"Date\": dates, \"Value\": synthetic_data})\n",
    "plt.plot(df[\"Date\"], df[\"Value\"])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('3 years of synthetic seasonal data')\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Date')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59f2b23",
   "metadata": {},
   "source": [
    "We can use the `statsmodels.graphics` library to uncover trends and seasonality in our data. The `plot_acf` (autocorrelation function plot) function shows you:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25e6526",
   "metadata": {},
   "outputs": [],
   "source": [
    "acf_plot = tsaplots.plot_acf(df[\"Value\"], lags=365, title='Autocorrelation plot')\n",
    "\n",
    "# Adjust the plot size\n",
    "acf_plot.set_figheight(5)\n",
    "acf_plot.set_figwidth(10)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029370d4",
   "metadata": {},
   "source": [
    "Values range from +1 to -1 and indicate how closely each value is correlated to past (lagged) values.\n",
    "- +1 at the start makes sense (it's closely correlated to itself).\n",
    "- After about 91.5 days the correlation is again strong, which makes sense because there are 4 cycles in one year.\n",
    "- After 45.5 days the correlation is strong but negative, corresponding to the troughs in the seasonality (again, 4 per year).\n",
    "- Correlations become weaker as the time difference becomes greater. This is due to the overall trend.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2603e4",
   "metadata": {},
   "source": [
    "## Demo 6: Using statsmodels.tsa to decompose a time series\n",
    "\n",
    "Statsmodels tsa can be used to decompose a time series into seasonal components. For synthetic data such as the one we used in demo 5, this works quite well.\n",
    "\n",
    "We'll use the `statsmodels.tsa.seasonal` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9088434d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Decompose the time series\n",
    "result = seasonal_decompose(df[\"Value\"], model=\"additive\", period=365)\n",
    "\n",
    "# Plot the decomposed components\n",
    "decomposed = result.plot()\n",
    "\n",
    "decomposed.set_figheight(8)\n",
    "decomposed.set_figwidth(10)\n",
    "decomposed.suptitle(\"Time Series Decomposition\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a07cab",
   "metadata": {},
   "source": [
    "As you can see, `statsmodels.tsa.seasonal` has uncovered the trend we put in as well as our seasonal trend. The results aren't perfect (we *know* our seasonal effect graph should be smooth) but close enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f05406",
   "metadata": {},
   "source": [
    "## Demo 7: Using statsmodels.tsa to analyse and decompose sunspot data\n",
    "\n",
    "Sunspots are famous for increasing and decreasing in intensity in 11 year cycles. You would therefore expect to be able to use autocorrelation function analysis to uncover underlying trends and patterns.\n",
    "Let's see if this is in fact possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abfdd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load the data and show it.\n",
    "df_sunspots = pd.read_csv('data/SN_m_tot_V2.0.txt',\n",
    "    sep='\\\\s+',\n",
    "    header=None,\n",
    "    names=[\n",
    "        \"year\",\n",
    "        \"month\",\n",
    "        \"decimal_date\",\n",
    "        \"sunspots\",\n",
    "        \"std_dev\",\n",
    "        \"observations\",\n",
    "        \"definitive\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "plt.plot(df_sunspots[\"decimal_date\"], df_sunspots[\"sunspots\"])\n",
    "plt.title('Sunspots')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f70f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create an CSV plot.\n",
    "acf_plot = tsaplots.plot_acf(\n",
    "    df_sunspots[\"sunspots\"], lags=30*12, title='Autocorrelation plot sunspot data (30 year lag)'\n",
    ")  # lets go back 30 years (12 months each, each data point is one month)\n",
    "\n",
    "\n",
    "# Adjust the plot size\n",
    "acf_plot.set_figheight(5)\n",
    "acf_plot.set_figwidth(10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400400d4",
   "metadata": {},
   "source": [
    "So far, so good. There seems to be a clear pattern in the data - as there should be: the cycle repeats every 11 years or 132 data points. The correlation is not strong, however - by the time the third cycle hits, it's less than 0.5.\n",
    "\n",
    "Let's see what `seasonal_decompose` can tell us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1001c561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a seasonal decompose of the sunspot data\n",
    "result = seasonal_decompose(df_sunspots[\"sunspots\"], model=\"additive\", period=365)\n",
    "\n",
    "decomposed = result.plot()\n",
    "decomposed.set_figheight(8)\n",
    "decomposed.set_figwidth(10)\n",
    "decomposed.suptitle(\"Sunspot time series decomposition\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966f848a",
   "metadata": {},
   "source": [
    "Whoops - that's terrible. Both the trend (a roughly 100 year cycle) and the residuals seem to contain obvious additional sine wave type cycles (obvious, that is, to us, if we squint). Clearly the underlying pattern is too complex for the decomposition algorithm to pick up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd044cc",
   "metadata": {},
   "source": [
    "## Demo 8: smoothing data using filters\n",
    "\n",
    "Often, small variations in data are not meaningful - they're \"noise\". This \"noise\" can, however, interfere with our analysis. Tools called \"filters\" can help \"filter out\" such \"noise\" so we can get a clearer picture of the data.\n",
    "\n",
    "Let's create a synthetic time series with some noise and then filter it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f138c64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a noisy sine wave for demonstration\n",
    "np.random.seed(0)  # For reproducibility\n",
    "time = np.linspace(0, 1, 200)  # 1 second, 200 samples\n",
    "frequency = 5  # Frequency of the sine wave\n",
    "amplitude = 1  # Amplitude of the sine wave\n",
    "noise = np.random.normal(0, 0.2, time.shape)  # Gaussian noise\n",
    "sine_wave = amplitude * np.sin(2 * np.pi * frequency * time)\n",
    "noisy_signal = sine_wave + noise\n",
    "\n",
    "pd.Series(noisy_signal).plot()\n",
    "plt.title('A noisy sine wave signal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0754cd",
   "metadata": {},
   "source": [
    "The `scipy.signal` library contains some useful filters. Here we will apply two of them: Savitzky-Golay and the Butterworth low pass filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fc482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Savitzky-Golay filter\n",
    "window_size = 51  # Window size should be odd\n",
    "poly_order = 3  # Polynomial order\n",
    "savitzky_golay_filtered = savgol_filter(noisy_signal, window_size, poly_order)\n",
    "\n",
    "# Apply Low-pass Butterworth filter\n",
    "cutoff_frequency = 0.1  # Cutoff frequency as a fraction of the sampling rate\n",
    "filter_order = 3  # Filter order\n",
    "b, a = butter(filter_order, cutoff_frequency, btype=\"low\", analog=False)\n",
    "butterworth_filtered = filtfilt(b, a, noisy_signal)\n",
    "\n",
    "# Plot the original and filtered signals\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(time, noisy_signal, label=\"Noisy signal\", color=\"lightgray\")\n",
    "plt.plot(time, sine_wave, label=\"Original sine wave\", linestyle=\"--\", color=\"black\")\n",
    "plt.plot(time, savitzky_golay_filtered, label=\"Savitzky-Golay Filtered\", color=\"red\")\n",
    "plt.plot(\n",
    "    time, butterworth_filtered, label=\"Low-pass Butterworth Filtered\", color=\"blue\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.title(\"Signal Smoothing with Savitzky-Golay and Low-pass Butterworth Filters\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f792e4c0",
   "metadata": {},
   "source": [
    "Clearly, the Butterworth filter especially can remove noise so well that you almost get the original sine wave back."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33965112",
   "metadata": {},
   "source": [
    "## Demo 9: smoothing sunspot data using filters\n",
    "\n",
    "Finally, let's see if smoothing the sunspot data using filters yields a better seasonality analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a2be1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a Butterworth filter to the sunspot data (because Butterworth performed to well in the previous demo)\n",
    "\n",
    "cutoff_frequency = 0.1  # Cutoff frequency as a fraction of the sampling rate\n",
    "filter_order = 3  # Filter order\n",
    "b, a = butter(filter_order, cutoff_frequency, btype=\"low\", analog=False)\n",
    "sunspots_filtered = filtfilt(b, a, df_sunspots[\"sunspots\"] )\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(df_sunspots[\"decimal_date\"], df_sunspots[\"sunspots\"], label=\"Sunspot data\", color=\"lightgray\")\n",
    "plt.plot(\n",
    "    df_sunspots[\"decimal_date\"], sunspots_filtered, label=\"Low-pass Butterworth Filtered\", color=\"blue\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.title(\"Sunspot data filtered using a Low-pass Butterworth filter\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93677f7",
   "metadata": {},
   "source": [
    "The graph certainly seems smoother. Let's see if the filtering we did has yielded data that is easier to decompose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdfecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = seasonal_decompose(sunspots_filtered, model=\"additive\", period=365)\n",
    "\n",
    "decomposed = result.plot()\n",
    "decomposed.set_figheight(8)\n",
    "decomposed.set_figwidth(10)\n",
    "decomposed.suptitle(\"Filtered sunspot time series decomposition\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6729357",
   "metadata": {},
   "source": [
    "Well, no. The residuals certainly appear less noisy, but they still contain cyclical data the algorithm doesn't catch.\n",
    "If you want, you can play around with the cutoff_frequency variable above to see if that changes things.\n",
    "Ultimately, however, filtering will only get you so far. If the data is complex, no amount of filtering is going to change that fact."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
